{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5a2feec",
   "metadata": {},
   "source": [
    "# IT Service Ticket Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbc77d3",
   "metadata": {},
   "source": [
    "## Navegação dos notebooks\n",
    "\n",
    "- `notebooks/analysis.ipynb`: análise exploratória (EDA).\n",
    "- `notebooks/classificators.ipynb`: testes de classificadores (TF-IDF, embeddings, RAG) com métricas no conjunto de teste.\n",
    "- `notebooks/main.ipynb`: visão geral, prompts, pipeline, avaliação final (usa o classificador escolhido).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96feb79",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Preparação dos Dados\n",
    "\n",
    "### Divisão em treino, teste e validação\n",
    "\n",
    "Usamos três conjuntos de dados:\n",
    "\n",
    "1. **Validação (balanceada)**: `VALIDATION_SIZE` tickets, com o mesmo número de exemplos por classe.\n",
    "2. **Treino/Teste**: o restante é dividido em **80% treino** e **20% teste**, com estratificação.\n",
    "\n",
    "**Por que assim?**\n",
    "- **Validação balanceada** garante comparação justa entre classes.\n",
    "- **Teste separado** permite comparar métodos antes da avaliação final.\n",
    "- **Treino maior** melhora a qualidade dos modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51074ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tickets: 47,837\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>connection with icon icon dear please setup ic...</td>\n",
       "      <td>Hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>work experience user work experience user hi w...</td>\n",
       "      <td>Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>requesting for meeting requesting meeting hi p...</td>\n",
       "      <td>Hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reset passwords for external accounts re expir...</td>\n",
       "      <td>Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mail verification warning hi has got attached ...</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document    Topic_group\n",
       "0  connection with icon icon dear please setup ic...       Hardware\n",
       "1  work experience user work experience user hi w...         Access\n",
       "2  requesting for meeting requesting meeting hi p...       Hardware\n",
       "3  reset passwords for external accounts re expir...         Access\n",
       "4  mail verification warning hi has got attached ...  Miscellaneous"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from classifier.data import load_dataset, train_test_validation_split\n",
    "\n",
    "# Carregar dataset\n",
    "df, classes = load_dataset()\n",
    "print(f\"Total de tickets: {len(df):,}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes (8):\n",
      "  - Access\n",
      "  - Administrative rights\n",
      "  - HR Support\n",
      "  - Hardware\n",
      "  - Internal Project\n",
      "  - Miscellaneous\n",
      "  - Purchase\n",
      "  - Storage\n"
     ]
    }
   ],
   "source": [
    "# Classes obtidas do dataset\n",
    "print(f\"Classes ({len(classes)}):\")\n",
    "for c in classes:\n",
    "    print(f\"  - {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino:     38,109 tickets\n",
      "Teste:      9,528 tickets\n",
      "Validação:  200 tickets\n",
      "\n",
      "Distribuição na validação:\n",
      "Topic_group\n",
      "Access                   25\n",
      "Administrative rights    25\n",
      "HR Support               25\n",
      "Hardware                 25\n",
      "Internal Project         25\n",
      "Miscellaneous            25\n",
      "Purchase                 25\n",
      "Storage                  25\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from classifier.config import VALIDATION_SIZE\n",
    "\n",
    "# Split: validação balanceada + treino/teste estratificados (80/20)\n",
    "train_df, test_df, validation_df = train_test_validation_split(\n",
    "    df,\n",
    "    validation_size=VALIDATION_SIZE,\n",
    ")\n",
    "\n",
    "print(f\"Treino:     {len(train_df):,} tickets\")\n",
    "print(f\"Teste:      {len(test_df):,} tickets\")\n",
    "print(f\"Validação:  {len(validation_df):,} tickets\")\n",
    "print(\"\\nDistribuição na validação:\")\n",
    "print(validation_df[\"Topic_group\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2383c59",
   "metadata": {},
   "source": [
    "## 2. Métodos de Classificação e Comparação\n",
    "\n",
    "Os experimentos de classificação estão em `notebooks/classificators.ipynb`, onde rodamos TF-IDF, embeddings e RAG no conjunto de teste.\n",
    "\n",
    "Modelo selecionado (teste):\n",
    "- **TF-IDF + LinearSVC** — accuracy **0.8637**, F1 macro **0.8641**, F1 weighted **0.8639**.\n",
    "\n",
    "As seções seguintes assumem este modelo como escolhido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Design dos Prompts\n",
    "\n",
    "O prompt é a interface entre nosso sistema e a LLM. Um bom design de prompt é crucial para obter classificações precisas e justificativas úteis.\n",
    "\n",
    "### Estrutura do Prompt\n",
    "\n",
    "O prompt é dividido em duas partes:\n",
    "\n",
    "1. **System prompt:** Define o papel da LLM (classificador), lista as classes válidas e especifica o formato de saída (JSON)\n",
    "\n",
    "2. **User prompt:** Contém o ticket a classificar e os exemplos de contexto\n",
    "\n",
    "### Parâmetros Configuráveis\n",
    "\n",
    "| Parâmetro | Descrição | Valor Padrão |\n",
    "|-----------|-----------|---------------|\n",
    "| `K_SIMILAR` | Número de tickets similares do RAG | 5 |\n",
    "| `reference_tickets` | Tickets representativos por classe | 1 por classe |\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "- **Mais exemplos similares:** Melhor contexto para classificação, mas aumenta tokens e custo\n",
    "- **Tickets de referência:** Garante diversidade de classes, essencial para justificativas comparativas\n",
    "- **Muitos exemplos:** Pode \"poluir\" o contexto e confundir a LLM\n",
    "\n",
    "Vamos visualizar como os prompts são gerados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.prompts import build_system_prompt, build_user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 1: System Prompt\n",
    "system_prompt = build_system_prompt(classes)\n",
    "print(\"=== SYSTEM PROMPT ===\\n\")\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 2: User Prompt COM tickets de referência\n",
    "user_prompt_with_refs = build_user_prompt(query, similar, representatives)\n",
    "print(\"=== USER PROMPT (com tickets de referência) ===\\n\")\n",
    "print(user_prompt_with_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 3: User Prompt SEM tickets de referência\n",
    "# Útil quando queremos usar menos tokens ou quando os similares já são suficientes\n",
    "user_prompt_no_refs = build_user_prompt(query, similar, reference_tickets=None)\n",
    "print(\"=== USER PROMPT (sem tickets de referência) ===\\n\")\n",
    "print(user_prompt_no_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação de Uso de Tokens\n",
    "\n",
    "A tabela abaixo mostra o impacto real de cada parâmetro no consumo de tokens. Isso ajuda a escolher a configuração ideal considerando o trade-off entre qualidade do contexto e custo/latência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Usar tokenizer cl100k_base (compatível com GPT-4, GPT-3.5-turbo, etc.)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Conta tokens usando o tokenizer cl100k_base.\"\"\"\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "# Comparar diferentes configurações de prompt\n",
    "# K=5 sem refs é a baseline (configuração mínima recomendada)\n",
    "configs = [\n",
    "    (\"K=5, sem refs\", build_user_prompt(query, similar[:5], None)),\n",
    "    (\"K=5, com refs\", build_user_prompt(query, similar[:5], representatives)),\n",
    "    (\"K=3, sem refs\", build_user_prompt(query, similar[:3], None)),\n",
    "    (\"K=3, com refs\", build_user_prompt(query, similar[:3], representatives)),\n",
    "    (\"K=1, sem refs\", build_user_prompt(query, similar[:1], None)),\n",
    "    (\"K=1, com refs\", build_user_prompt(query, similar[:1], representatives)),\n",
    "]\n",
    "\n",
    "# Tabela comparativa\n",
    "system_tokens = count_tokens(system_prompt)\n",
    "print(f\"System prompt: {system_tokens} tokens (fixo)\\n\")\n",
    "print(f\"{'Configuração':<16} | {'User Prompt':>12} | {'Total':>8} | {'vs baseline':>12}\")\n",
    "print(\"-\" * 58)\n",
    "baseline = None\n",
    "for name, prompt in configs:\n",
    "    user_tokens = count_tokens(prompt)\n",
    "    total = system_tokens + user_tokens\n",
    "    if baseline is None:\n",
    "        baseline = total\n",
    "        diff = \"(base)\"\n",
    "    else:\n",
    "        diff = f\"{(total - baseline) / baseline * 100:+.0f}%\"\n",
    "    print(f\"{name:<16} | {user_tokens:>12} | {total:>8} | {diff:>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classificação com LLM\n",
    "\n",
    "Com o prompt construído, enviamos para a LLM. O sistema suporta qualquer API compatível com OpenAI configurada via variáveis de ambiente.\n",
    "\n",
    "**Requer:** variáveis de ambiente `LLM_BASE_URL` e `LLM_MODEL` (ver `.env.example`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.llm import TicketClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar classificador\n",
    "classifier = TicketClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classificar o ticket de teste\n",
    "details = classifier.classify(query, similar, classes, reference_tickets=representatives)\n",
    "\n",
    "print(f\"Classe real: {true_class}\")\n",
    "print(f\"Classe predita: {details.result.classe}\")\n",
    "print(f\"Justificativa: {details.result.justificativa}\")\n",
    "if details.reasoning:\n",
    "    print(f\"\\nReasoning:\")\n",
    "    print(details.reasoning)\n",
    "print(f\"\\nCorreto: {details.result.classe == true_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo de Resposta com Reasoning\n",
    "\n",
    "**Sem reasoning:**\n",
    "```json\n",
    "{\n",
    "  \"classe\": \"Access\",\n",
    "  \"justificativa\": \"O ticket menciona problemas com cartão de acesso.\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Com reasoning:**\n",
    "```json\n",
    "{\n",
    "  \"classe\": \"Access\",\n",
    "  \"justificativa\": \"O ticket descreve problemas com 'card key' e 'door access'...\",\n",
    "  \"reasoning\": \"Analisei o ticket e identifiquei palavras-chave como 'card key', 'door' e 'access'...\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### Configuração\n",
    "\n",
    "```bash\n",
    "# .env\n",
    "LLM_MODEL=xiaomi/mimo-v2-flash:free\n",
    "LLM_REASONING_EFFORT=medium  # low, medium, ou high\n",
    "```\n",
    "\n",
    "Via CLI:\n",
    "```bash\n",
    "uv run python main.py --reasoning medium\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de Reasoning\n",
    "\n",
    "Ativamos o **modo reasoning** do modelo (`LLM_REASONING_EFFORT=medium`) para melhorar a qualidade das classificações.\n",
    "\n",
    "#### Benefícios\n",
    "\n",
    "| Aspecto | Impacto do Reasoning |\n",
    "|---------|---------------------|\n",
    "| **Casos ambíguos** | O modelo \"pensa\" antes de decidir, analisando nuances |\n",
    "| **Justificativas** | Explicações mais detalhadas do processo de decisão |\n",
    "| **Precisão** | Reduz erros entre classes similares |\n",
    "| **Transparência** | Campo `reasoning` mostra o raciocínio completo |\n",
    "\n",
    "#### Trade-offs\n",
    "\n",
    "Ao ativar reasoning, enfrentamos:\n",
    "| Aspecto | Trade-off |\n",
    "|---------|-----------|\n",
    "| **Tokens** | Aumento no número de tokens e custo por chamada |\n",
    "| **Latência** | Respostas mais lentas devido ao processamento adicional |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Escolha do Modelo LLM\n",
    "\n",
    "Foi escolhido o modelo **MIMO v2 Flash** da Xiaomi, baseado nos seguintes critérios:\n",
    "\n",
    "- Disponível gratuitamente via OpenRouter\n",
    "- Suporte nativo a reasoning\n",
    "- Performance competitiva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitetura da Solução\n",
    "\n",
    "O diagrama abaixo ilustra o fluxo completo de classificação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Criar diagrama de arquitetura\n",
    "fig = go.Figure()\n",
    "\n",
    "# Definir as etapas do pipeline\n",
    "steps = [\n",
    "    (\"Ticket\", \"#e1f5fe\", \"Texto de entrada\"),\n",
    "    (\"Embedding\", \"#fff3e0\", \"all-MiniLM-L6-v2\\n384 dimensões\"),\n",
    "    (\"Retrieval\", \"#e8f5e9\", \"K similares +\\nRepresentativos\"),\n",
    "    (\"Prompt\", \"#fce4ec\", \"System + User\\n+ Contexto RAG\"),\n",
    "    (\"LLM\", \"#f3e5f5\", \"API OpenAI-compatible\\n+ Retry JSON\"),\n",
    "    (\"Output\", \"#e0f2f1\", '{\"classe\": \"...\",\\n\"justificativa\": \"...\"}'),\n",
    "]\n",
    "\n",
    "# Posições\n",
    "x_positions = list(range(len(steps)))\n",
    "y_pos = 0.5\n",
    "\n",
    "# Adicionar caixas e textos\n",
    "for i, (name, color, desc) in enumerate(steps):\n",
    "    # Caixa\n",
    "    fig.add_shape(\n",
    "        type=\"rect\",\n",
    "        x0=i - 0.4, x1=i + 0.4,\n",
    "        y0=0.2, y1=0.8,\n",
    "        fillcolor=color,\n",
    "        line=dict(color=\"#333\", width=2),\n",
    "    )\n",
    "    # Nome da etapa\n",
    "    fig.add_annotation(\n",
    "        x=i, y=0.65,\n",
    "        text=f\"<b>{name}</b>\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=14),\n",
    "    )\n",
    "    # Descrição\n",
    "    fig.add_annotation(\n",
    "        x=i, y=0.38,\n",
    "        text=desc,\n",
    "        showarrow=False,\n",
    "        font=dict(size=10),\n",
    "        align=\"center\",\n",
    "    )\n",
    "    # Seta para próxima etapa\n",
    "    if i < len(steps) - 1:\n",
    "        fig.add_annotation(\n",
    "            x=i + 0.5, y=0.5,\n",
    "            ax=i + 0.42, ay=0.5,\n",
    "            xref=\"x\", yref=\"y\",\n",
    "            axref=\"x\", ayref=\"y\",\n",
    "            showarrow=True,\n",
    "            arrowhead=2,\n",
    "            arrowsize=1.5,\n",
    "            arrowcolor=\"#333\",\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text=\"Arquitetura do Pipeline RAG\", x=0.5, font=dict(size=16)),\n",
    "    xaxis=dict(visible=False, range=[-0.6, len(steps) - 0.4]),\n",
    "    yaxis=dict(visible=False, range=[0, 1]),\n",
    "    height=250,\n",
    "    margin=dict(l=20, r=20, t=50, b=20),\n",
    "    plot_bgcolor=\"white\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pipeline Completo com LangGraph\n",
    "\n",
    "O LangGraph orquestra todo o fluxo de classificação em um grafo de estados com 4 nós:\n",
    "\n",
    "| Nó | Função |\n",
    "|----|--------|\n",
    "| **embed** | Gera embedding do ticket (384 dimensões) |\n",
    "| **retrieve** | Busca K tickets similares usando o embedding |\n",
    "| **build_prompt** | Constrói system e user prompts com contexto RAG |\n",
    "| **classify** | Chama a LLM e processa a resposta JSON |\n",
    "\n",
    "Abaixo visualizamos a estrutura do grafo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from classifier.graph import create_graph\n",
    "\n",
    "# Criar o grafo para visualização\n",
    "pipeline = create_graph(retriever, classifier, classes, representatives)\n",
    "\n",
    "# Visualizar a estrutura do grafo LangGraph\n",
    "display(Image(pipeline.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.graph import classify_ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classificar usando o pipeline completo\n",
    "# A função classify_ticket encapsula todo o fluxo: retrieve → classify\n",
    "details_graph = classify_ticket(\n",
    "    ticket=query,\n",
    "    retriever=retriever,\n",
    "    classifier=classifier,\n",
    "    classes=classes,\n",
    "    reference_tickets=representatives,\n",
    ")\n",
    "\n",
    "print(f\"Classe real: {true_class}\")\n",
    "print(f\"Classe predita: {details_graph.result.classe}\")\n",
    "print(f\"Justificativa: {details_graph.result.justificativa}\")\n",
    "if details_graph.reasoning:\n",
    "    print(f\"\\nReasoning:\")\n",
    "    print(details_graph.reasoning)\n",
    "print(f\"\\nCorreto: {details_graph.result.classe == true_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Avaliação do Classificador\n",
    "\n",
    "Agora vamos avaliar o desempenho do classificador nos 200 tickets de teste.\n",
    "\n",
    "### Métricas Utilizadas\n",
    "\n",
    "- **Accuracy:** Proporção de classificações corretas\n",
    "- **F1 Macro:** Média não-ponderada do F1 por classe (trata todas as classes igualmente, importante para datasets desbalanceados)\n",
    "- **Cohen's Kappa:** Mede concordância além do acaso (valores próximos de 1 indicam excelente concordância)\n",
    "- **MCC (Matthews Correlation Coefficient):** Métrica robusta para classificação multi-classe\n",
    "- **Confusion Matrix:** Visualiza erros de classificação entre classes\n",
    "\n",
    "### Tratamento de Erros\n",
    "\n",
    "LLMs podem ocasionalmente retornar JSON malformado, ou a API pode apresentar erros. Para lidar com isso:\n",
    "\n",
    "**Erros de JSON (parsing):**\n",
    "1. **Primeira tentativa:** Envia o prompt normal para a LLM\n",
    "2. **Se JSON inválido:** Continua a conversa adicionando a resposta do assistant e um prompt de correção:\n",
    "   ```\n",
    "   Sua resposta anterior não está no formato JSON válido.\n",
    "   Por favor, responda APENAS com JSON válido no formato:\n",
    "   {\"classe\": \"<categoria>\", \"justificativa\": \"<explicação>\"}\n",
    "   ```\n",
    "3. **Segunda tentativa:** LLM recebe o contexto completo da conversa e tenta corrigir\n",
    "4. **Se falhar novamente:** O ticket é marcado como erro e reportado separadamente\n",
    "\n",
    "**Erros de API:**\n",
    "- Erros de autenticação (401), rate limit (429), ou outros erros da API são capturados\n",
    "- O ticket é marcado como erro com o motivo específico\n",
    "- A avaliação continua com os próximos tickets\n",
    "\n",
    "Este approach garante que:\n",
    "- Um erro pontual não interrompe toda a avaliação\n",
    "- A LLM tem chance de se corrigir com contexto adicional\n",
    "- Erros persistentes são documentados para análise posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.metrics import evaluate, print_report, plot_confusion_matrix, plot_per_class_metrics\n",
    "from classifier.runner import classify_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classificar todos os tickets de teste usando classify_batch()\n",
    "results, classification_errors, total_tokens = classify_batch(\n",
    "    test_df=test_df,\n",
    "    retriever=retriever,\n",
    "    classifier=classifier,\n",
    "    classes=classes,\n",
    "    reference_tickets=representatives,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Extrair y_true e y_pred para métricas\n",
    "y_true = [r[\"true_class\"] for r in results]\n",
    "y_pred = [r[\"predicted_class\"] for r in results]\n",
    "\n",
    "print(f\"\\nClassificação concluída!\")\n",
    "print(f\"Classificados com sucesso: {len(results)}\")\n",
    "if classification_errors:\n",
    "    print(f\"Erros de classificação (JSON inválido): {len(classification_errors)}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TOKEN USAGE SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Prompt tokens:     {total_tokens.prompt_tokens:,}\")\n",
    "print(f\"Completion tokens: {total_tokens.completion_tokens:,}\")\n",
    "print(f\"Total tokens:      {total_tokens.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas\n",
    "metrics = evaluate(y_true, y_pred, classes)\n",
    "print_report(metrics, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretação das Métricas Gerais\n",
    "\n",
    "Os resultados da avaliação indicam um classificador com **performance sólida**:\n",
    "\n",
    "| Métrica | Valor | Interpretação |\n",
    "|---------|-------|---------------|\n",
    "| **Accuracy** | 76.5% | ~3 em 4 tickets classificados corretamente |\n",
    "| **F1 Macro** | 75.8% | Performance equilibrada entre classes (não favorece majoritárias) |\n",
    "| **Cohen's Kappa** | 0.73 | Concordância **substancial** (escala: >0.6 bom, >0.8 excelente) |\n",
    "| **MCC** | 0.74 | Confirma robustez - métrica equilibrada para problemas multiclasse |\n",
    "\n",
    "**Destaques por classe:**\n",
    "- **Maior recall:** Access (92%) e Internal Project (92%) - o modelo raramente perde tickets dessas classes\n",
    "- **Maior precision:** Purchase (95%) - quando prediz Purchase, quase sempre acerta\n",
    "- **Classe problemática:** Administrative rights (F1=49%) - recall de apenas 36%, indicando que muitos tickets dessa classe são classificados incorretamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: RAG-only com votacao ponderada\n",
    "\n",
    "Avaliamos o desempenho da classificacao direta via retrieval, sem LLM,\n",
    "usando a mesma configuracao de K similares.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação: RAG+LLM vs Baseline (Weighted Vote)\n",
    "\n",
    "| Aspecto | RAG + LLM | Baseline |\n",
    "|---------|-----------|----------|\n",
    "| Accuracy | 76.5% | **77.5%** |\n",
    "| F1 Macro | 75.8% | **77.6%** |\n",
    "| Tokens consumidos | 366k | 0 |\n",
    "| Tempo de execução | ~45 min | ~2 seg |\n",
    "| Justificativas | **Sim** | Não |\n",
    "\n",
    "**Insights:**\n",
    "\n",
    "1. **Métricas numéricas:** A baseline supera ligeiramente o RAG+LLM. Isso indica que o retriever já captura bem a semântica dos tickets - a LLM não adiciona acurácia significativa.\n",
    "\n",
    "2. **Valor agregado da LLM:** O diferencial está nas **justificativas em linguagem natural**. Para sistemas de suporte, explicar *por que* um ticket foi classificado é tão importante quanto a classificação em si.\n",
    "\n",
    "3. **Trade-off custo/benefício:** A LLM consome tokens e tempo, mas oferece interpretabilidade. Em produção, uma abordagem híbrida pode ser ideal: usar weighted vote para triagem rápida e LLM apenas quando justificativa for necessária.\n",
    "\n",
    "4. **Erros similares:** Ambos os métodos têm dificuldade com Administrative rights, sugerindo que o problema está na representação semântica dessa classe, não na capacidade da LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.config import K_SIMILAR\n",
    "from tqdm import tqdm\n",
    "\n",
    "weighted_preds = []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Classificando (weighted vote)\"):\n",
    "    similar = retriever.retrieve(row[\"Document\"], k=K_SIMILAR)\n",
    "    vote = retriever.weighted_vote(similar)\n",
    "    weighted_preds.append(vote[\"predicted_class\"])\n",
    "\n",
    "y_true_weighted = test_df[\"Topic_group\"].tolist()\n",
    "metrics_weighted = evaluate(y_true_weighted, weighted_preds, classes)\n",
    "print_report(metrics_weighted, classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Graficos (weighted vote)\n",
    "\n",
    "Visualizacao por classe e confusion matrix para a baseline RAG-only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficos por classe - weighted vote\n",
    "plot_per_class_metrics(\n",
    "    y_true_weighted,\n",
    "    weighted_preds,\n",
    "    classes,\n",
    "    title=\"Precision, Recall e F1-Score por Classe - Weighted Vote\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix - weighted vote\n",
    "plot_confusion_matrix(\n",
    "    metrics_weighted[\"confusion_matrix\"],\n",
    "    classes,\n",
    "    title=\"Confusion Matrix - Weighted Vote\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas por Classe\n",
    "\n",
    "O gráfico abaixo mostra precision, recall e F1-score para cada classe, facilitando a identificação visual de quais classes têm melhor/pior desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de métricas por classe\n",
    "plot_per_class_metrics(y_true, y_pred, classes, title=\"Precision, Recall e F1-Score por Classe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar confusion matrix\n",
    "plot_confusion_matrix(metrics[\"confusion_matrix\"], classes, title=\"Confusion Matrix - Classificador RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise da Matriz de Confusão\n",
    "\n",
    "A matriz de confusão normalizada revela padrões importantes:\n",
    "\n",
    "**Classes bem separadas (diagonal forte):**\n",
    "- **Internal Project (92%):** Tickets com códigos de projeto são distintivos\n",
    "- **Purchase (84%):** Vocabulário específico (PO, purchase order) facilita identificação\n",
    "- **Access (92%):** Padrões como \"access card\", \"password reset\" são claros\n",
    "\n",
    "**Principais confusões:**\n",
    "\n",
    "| Confusão | Ocorrências | Possível causa |\n",
    "|----------|-------------|----------------|\n",
    "| Admin rights → Hardware | 12 (48%) | Tickets de \"upgrade\" e \"update\" aparecem em ambas |\n",
    "| Miscellaneous → HR Support | 4 | Tickets administrativos genéricos |\n",
    "| Storage → Access | 3 | Ambas tratam de \"acesso\" a recursos |\n",
    "\n",
    "**Observações:**\n",
    "- **Administrative rights** é a classe mais problemática: apenas 36% de recall\n",
    "- **Miscellaneous** funciona como classe \"catch-all\", capturando tickets que não se encaixam claramente em outras categorias\n",
    "- A confusão Hardware/Admin rights sugere que upgrades de **software** (Admin rights) são confundidos com upgrades de **hardware**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Normalizada\n",
    "\n",
    "A versão normalizada mostra percentagens em vez de contagens absolutas, facilitando a identificação de taxas de erro por classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix normalizada (percentagens)\n",
    "plot_confusion_matrix(\n",
    "    metrics[\"confusion_matrix\"], \n",
    "    classes, \n",
    "    title=\"Confusion Matrix - Normalizada por True Label\",\n",
    "    normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de Erros\n",
    "\n",
    "Existem dois tipos de erros na avaliação:\n",
    "\n",
    "1. **Erros de classificação:** A LLM retornou um JSON válido, mas a classe predita difere da classe real\n",
    "2. **Erros de parsing:** A LLM não retornou JSON válido mesmo após retry (tickets não incluídos nas métricas)\n",
    "\n",
    "Abaixo analisamos os erros de classificação para entender os padrões de confusão entre classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar erros\n",
    "errors = [r for r in results if not r[\"correct\"]]\n",
    "print(f\"Total de erros: {len(errors)} de {len(results)} ({100 * len(errors) / len(results):.1f}%)\\n\")\n",
    "\n",
    "# Mostrar alguns exemplos de erros\n",
    "print(\"=\" * 80)\n",
    "print(\"EXEMPLOS DE ERROS DE CLASSIFICAÇÃO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, error in enumerate(errors[:5], 1):\n",
    "    print(f\"\\n--- Erro {i} ---\")\n",
    "    print(f\"Classe real:     {error['true_class']}\")\n",
    "    print(f\"Classe predita:  {error['predicted_class']}\")\n",
    "    print(f\"Justificativa:   {error['justification']}\")\n",
    "    print(f\"Ticket:          {error['ticket'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padrões nos Erros de Classificação\n",
    "\n",
    "Analisando os 47 erros (23.5% do total), identificamos padrões recorrentes:\n",
    "\n",
    "**1. Administrative rights → Hardware (12 erros)**\n",
    "Exemplos mostram tickets sobre \"upgrade\", \"update\", \"performance issues\" que semanticamente poderiam pertencer a ambas:\n",
    "- *\"oracle upgrade zones\"* → classificado como Hardware\n",
    "- *\"windows upgrade failed\"* → classificado como Hardware\n",
    "- *\"performance issues after upgrading endpoint tool\"* → classificado como Hardware\n",
    "\n",
    "**2. Miscellaneous → HR Support (4 erros)**\n",
    "Tickets administrativos genéricos que mencionam aprovações ou configurações de usuário.\n",
    "\n",
    "**3. Purchase → Internal Project (3 erros)**\n",
    "Confusão entre códigos de projeto e ordens de compra (PO), já que ambos envolvem códigos e formulários.\n",
    "\n",
    "**Insight principal:**\n",
    "Os limites semânticos entre certas classes são **inerentemente ambíguos**. Um ticket sobre \"upgrade do Windows\" pode ser:\n",
    "- **Administrative rights:** se for sobre permissões para fazer upgrade\n",
    "- **Hardware:** se for sobre o processo técnico de upgrade\n",
    "\n",
    "Essa ambiguidade está no próprio dataset, não apenas no classificador.\n",
    "\n",
    "**Erros coerentes com julgamento humano:**\n",
    "As justificativas geradas pelo modelo para os erros são **razoáveis e bem fundamentadas**. Um exemplo ilustrativo: o ticket *\"performance issues... after upgrading endpoint tool takes up cpu makes work impossible android studio...\"* (classe real: Administrative rights) foi classificado como Hardware com a justificativa *\"O ticket relata problemas de desempenho devido ao consumo excessivo de CPU pela ferramenta de endpoint após uma atualização, impossibilitando o trabalho em ambientes como Android Studio, o que está relacionado a recursos de hardware\"*. A menção a CPU, problemas de desempenho e impossibilidade de trabalhar são sintomas tipicamente associados a limitações de hardware — um classificador humano facilmente chegaria à mesma conclusão. Os erros não são aleatórios ou absurdos — são confusões compreensíveis em zonas de fronteira semântica, onde a própria definição das classes no dataset é ambígua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição de erros por classe\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Distribuição de erros por classe real:\")\n",
    "error_by_true = Counter(e[\"true_class\"] for e in errors)\n",
    "for cls, count in sorted(error_by_true.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {cls}: {count} erros\")\n",
    "\n",
    "print(\"\\nConfusões mais comuns (real → predito):\")\n",
    "confusion_pairs = Counter((e[\"true_class\"], e[\"predicted_class\"]) for e in errors)\n",
    "for (true, pred), count in confusion_pairs.most_common(10):\n",
    "    print(f\"  {true} → {pred}: {count}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relação entre Similaridade e Acertos\n",
    "\n",
    "A análise dos scores de similaridade revela uma correlação clara:\n",
    "\n",
    "| Predições | Similaridade Média | Similaridade Mediana |\n",
    "|-----------|-------------------|---------------------|\n",
    "| **Corretas** | 0.776 | 0.778 |\n",
    "| **Incorretas** | 0.674 | 0.671 |\n",
    "\n",
    "**Diferença: ~13% menor para predições incorretas**\n",
    "\n",
    "**Interpretação:**\n",
    "1. Quando o retriever encontra tickets **muito similares** (score alto), a classificação tende a ser correta\n",
    "2. Tickets com **baixa similaridade** aos exemplos do treino são mais propensos a erros\n",
    "3. Esses tickets \"difíceis\" provavelmente são:\n",
    "   - Atípicos ou outliers dentro de sua classe\n",
    "   - Semanticamente ambíguos (pertencem a zonas de fronteira entre classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuição de Erros por Classe\n",
    "\n",
    "Visualização gráfica dos erros por classe real, facilitando a identificação de quais classes são mais difíceis de classificar corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Gráfico de erros por classe\n",
    "if errors:\n",
    "    error_data = [{\"Classe\": cls, \"Erros\": count} for cls, count in error_by_true.items()]\n",
    "    fig = px.bar(\n",
    "        error_data, \n",
    "        x=\"Erros\", \n",
    "        y=\"Classe\", \n",
    "        orientation=\"h\",\n",
    "        title=\"Erros por Classe Real\",\n",
    "        color=\"Erros\",\n",
    "        color_continuous_scale=\"Reds\"\n",
    "    )\n",
    "    fig.update_layout(height=350, showlegend=False)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Nenhum erro de classificação para exibir!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de Scores de Similaridade\n",
    "\n",
    "Comparação dos scores de similaridade entre predições corretas e incorretas. Scores mais baixos em predições incorretas indicam que o retriever teve dificuldade em encontrar exemplos relevantes. Se os scores são similares, o problema está na classificação do LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calcular média dos scores de similaridade por predição\n",
    "def mean_similarity(result):\n",
    "    scores = [t['score'] for t in result['similar_tickets']]\n",
    "    return np.mean(scores) if scores else 0\n",
    "\n",
    "scores_correct = [mean_similarity(r) for r in results if r['correct']]\n",
    "scores_incorrect = [mean_similarity(r) for r in results if not r['correct']]\n",
    "\n",
    "# Box plot comparativo\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(y=scores_correct, name='Corretos', marker_color='#2ca02c'))\n",
    "fig.add_trace(go.Box(y=scores_incorrect, name='Incorretos', marker_color='#d62728'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Distribuição de Scores de Similaridade (Média dos K Similares)',\n",
    "    yaxis_title='Score de Similaridade',\n",
    "    height=400\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Estatísticas\n",
    "print(f\"Corretos:   média={np.mean(scores_correct):.3f}, mediana={np.median(scores_correct):.3f}, n={len(scores_correct)}\")\n",
    "if scores_incorrect:\n",
    "    print(f\"Incorretos: média={np.mean(scores_incorrect):.3f}, mediana={np.median(scores_incorrect):.3f}, n={len(scores_incorrect)}\")\n",
    "else:\n",
    "    print(\"Nenhuma predição incorreta para análise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de Similaridade por Classe em Predições Erradas\n",
    "\n",
    "Para cada predição incorreta, comparamos a similaridade do ticket com:\n",
    "- **Classe predita**: A classe que o modelo escolheu incorretamente\n",
    "- **Classe real**: A classe correta do ticket\n",
    "\n",
    "Isso ajuda a entender se o erro foi \"justificado\" pelo RAG (similaridade alta com a classe predita) ou se o LLM cometeu um erro apesar dos sinais do RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de similaridade por classe para predições erradas\n",
    "if errors:\n",
    "    # Para cada erro, calcular similaridade com classe predita e classe real\n",
    "    similarity_data = []\n",
    "\n",
    "    for error in errors:\n",
    "        ticket_text = error['ticket']\n",
    "        pred_class = error['predicted_class']\n",
    "        true_class = error['true_class']\n",
    "\n",
    "        # Similaridade com classe predita\n",
    "        pred_sim = retriever.compute_class_similarity(ticket_text, pred_class, k=5)\n",
    "        # Similaridade com classe real\n",
    "        true_sim = retriever.compute_class_similarity(ticket_text, true_class, k=5)\n",
    "\n",
    "        similarity_data.append({\n",
    "            'ticket': ticket_text[:100] + '...',\n",
    "            'pred_class': pred_class,\n",
    "            'true_class': true_class,\n",
    "            'similarity_pred': pred_sim['mean_score'],\n",
    "            'similarity_true': true_sim['mean_score'],\n",
    "            'difference': pred_sim['mean_score'] - true_sim['mean_score'],\n",
    "        })\n",
    "\n",
    "    sim_df = pd.DataFrame(similarity_data)\n",
    "\n",
    "    # Visualização: Box plot comparando as duas similaridades\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Box(\n",
    "        y=sim_df['similarity_pred'],\n",
    "        name='Classe Predita',\n",
    "        marker_color='#d62728'\n",
    "    ))\n",
    "    fig.add_trace(go.Box(\n",
    "        y=sim_df['similarity_true'],\n",
    "        name='Classe Real',\n",
    "        marker_color='#2ca02c'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Similaridade por Classe em Predições Erradas',\n",
    "        yaxis_title='Score de Similaridade (média top-K)',\n",
    "        height=400\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # Estatísticas\n",
    "    print(f\"Similaridade com classe predita: média={sim_df['similarity_pred'].mean():.3f}, \"\n",
    "          f\"mediana={sim_df['similarity_pred'].median():.3f}\")\n",
    "    print(f\"Similaridade com classe real:   média={sim_df['similarity_true'].mean():.3f}, \"\n",
    "          f\"mediana={sim_df['similarity_true'].median():.3f}\")\n",
    "\n",
    "    # Análise: Quantos erros têm maior similaridade com a predita?\n",
    "    higher_pred_sim = (sim_df['similarity_pred'] > sim_df['similarity_true']).sum()\n",
    "    print(f\"\\nErros com maior similaridade à classe predita: {higher_pred_sim}/{len(errors)} \"\n",
    "          f\"({100*higher_pred_sim/len(errors):.1f}%)\")\n",
    "else:\n",
    "    print(\"Nenhum erro de classificação para análise de similaridade.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusão automática com LLM\n",
    "\n",
    "Nesta etapa, usamos um prompt de avaliação final para sintetizar os resultados.\n",
    "Ela compõe o processo de solução como a última fase de interpretação dos resultados,\n",
    "logo após o cálculo das métricas e a análise de erros.\n",
    "Ele recebe:\n",
    "- métricas globais e por classe\n",
    "- matriz de confusão e principais confusões\n",
    "- amostras de erros (até 20), com justificativa e tickets similares do RAG\n",
    "- configuração da execução (k, modelo, random_state)\n",
    "\n",
    "O objetivo é gerar um texto técnico com interpretação de desempenho, padrões de erro,\n",
    "papel do RAG e recomendações de melhorias, sem inventar números.\n",
    "\n",
    "A seguir, geramos a conclusão com os dados desta execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.conclusion import (\n",
    "    build_conclusion_payload,\n",
    "    build_conclusion_system_prompt,\n",
    "    build_conclusion_user_prompt,\n",
    ")\n",
    "from classifier.config import EMBEDDING_MODEL, K_SIMILAR, RANDOM_STATE\n",
    "from classifier.llm import ConclusionError\n",
    "\n",
    "payload = build_conclusion_payload(\n",
    "    dataset=\"dataset.csv\",\n",
    "    classes=classes,\n",
    "    test_size=len(test_df),\n",
    "    k_similar=K_SIMILAR,\n",
    "    use_references=bool(representatives),\n",
    "    embedding_model=EMBEDDING_MODEL,\n",
    "    llm_model=classifier.model,\n",
    "    random_state=RANDOM_STATE,\n",
    "    classifications=results,\n",
    "    errors=classification_errors,\n",
    "    metrics=metrics,\n",
    "    token_usage=total_tokens,\n",
    "    max_misclassified=20,\n",
    ")\n",
    "\n",
    "system_prompt = build_conclusion_system_prompt()\n",
    "user_prompt = build_conclusion_user_prompt(payload)\n",
    "\n",
    "try:\n",
    "    conclusion_text, conclusion_usage = classifier.generate_conclusion(\n",
    "        system_prompt=system_prompt,\n",
    "        user_prompt=user_prompt,\n",
    "    )\n",
    "    print(conclusion_text)\n",
    "    print(f\"\\nTokens usados (conclusão): {conclusion_usage.total_tokens:,}\")\n",
    "except ConclusionError as exc:\n",
    "    print(f\"Falha ao gerar conclusão: {exc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Considerações Finais\n",
    "\n",
    "Com base nas métricas globais e por classe apresentadas acima, o pipeline RAG+LLM demonstra que o\n",
    "retrieval é um componente determinante para a qualidade da classificação. Quando o retriever\n",
    "recupera exemplos consistentes, a predição tende a ser estável e a justificativa permanece coerente\n",
    "com os sinais do ticket.\n",
    "\n",
    "A análise da matriz de confusão e dos erros indica que as classes com maior sobreposição semântica\n",
    "são as mais suscetíveis a confusão. Isso sugere que melhorias no retriever (modelo de embedding,\n",
    "normalização e seleção de K) e no prompt podem reduzir ambiguidades e elevar o recall das classes\n",
    "mais críticas.\n",
    "\n",
    "A conclusão automática do LLM é uma etapa adicional para sintetizar os achados, mas não substitui\n",
    "a análise manual: ela funciona como um resumo guiado pelos dados fornecidos e pode acelerar a\n",
    "interpretação técnica, mantendo a revisão humana como referência principal.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ticket-classifier (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
