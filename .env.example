# LLM Provider Configuration
# Works with any OpenAI-compatible API

# Default (recommended): Ollama (local)
LLM_BASE_URL=http://localhost:11434/v1
LLM_MODEL=gemma2:2b
# Ollama doesn't require an API key, but some OpenAI-compatible clients expect a non-empty value.
LLM_API_KEY=ollama

# Reasoning (optional)- Uncomment to enable LLM reasoning/thinking mode (if supported by the model)
# Values: low, medium, high
# LLM_REASONING_EFFORT=medium
