{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT Service Ticket Classification\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Desenvolver um sistema de classificação automática de tickets de suporte de TI que:\n",
    "- **Entrada:** texto do ticket (string)\n",
    "- **Saída:** `{\"classe\": \"...\", \"justificativa\": \"...\"}`\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Utilizamos o dataset [IT Service Ticket Classification](https://www.kaggle.com/datasets/adisongoh/it-service-ticket-classification-dataset), que contém ~48.000 tickets de suporte de TI rotulados em 8 categorias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Análise Exploratória dos Dados\n",
    "\n",
    "Antes de implementar o sistema de classificação, é essencial compreender a estrutura dos dados, a distribuição das classes e as características dos textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "print(f\"Total de tickets: {len(df):,}\")\n",
    "print(f\"Colunas: {list(df.columns)}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dataset possui duas colunas:\n",
    "- **Document**: O texto do ticket de suporte\n",
    "- **Topic_group**: A categoria/classe do ticket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Qualidade dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Informações do DataFrame:\")\n",
    "print(\"-\" * 40)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valores nulos por coluna:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal de valores nulos: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valores duplicados:\")\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Tickets duplicados: {duplicates} ({duplicates/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Distribuição das Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df[\"Topic_group\"].value_counts().reset_index()\n",
    "class_counts.columns = [\"Classe\", \"Quantidade\"]\n",
    "class_counts[\"Percentual\"] = (class_counts[\"Quantidade\"] / len(df) * 100).round(2)\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    class_counts,\n",
    "    x=\"Classe\",\n",
    "    y=\"Quantidade\",\n",
    "    color=\"Classe\",\n",
    "    title=\"Distribuição de Tickets por Classe\",\n",
    "    text=\"Quantidade\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2\n",
    ")\n",
    "fig.update_traces(textposition=\"outside\")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Classe\",\n",
    "    yaxis_title=\"Quantidade de Tickets\",\n",
    "    showlegend=False,\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(\n",
    "    class_counts,\n",
    "    values=\"Quantidade\",\n",
    "    names=\"Classe\",\n",
    "    title=\"Proporção de Tickets por Classe\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2,\n",
    "    hole=0.3\n",
    ")\n",
    "fig.update_traces(textposition=\"inside\", textinfo=\"percent+label\")\n",
    "fig.update_layout(height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observações sobre a distribuição\n",
    "\n",
    "- Dataset **desbalanceado**: Hardware e HR Support representam ~51% dos dados\n",
    "- As classes menores (Administrative rights, Internal Project) têm menos de 5% cada\n",
    "- Total de **8 classes** distintas para classificação\n",
    "\n",
    "#### Implicações para o modelo de classificação\n",
    "\n",
    "1. **Amostragem para avaliação**: Para avaliar em 200 tickets, devemos usar amostragem estratificada para garantir representação de todas as classes.\n",
    "\n",
    "2. **RAG e exemplos**: O retriever terá mais exemplos de classes majoritárias. Isso pode ser benéfico (mais contexto) ou problemático (viés)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Análise do Texto dos Tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_length\"] = df[\"Document\"].str.len()\n",
    "df[\"word_count\"] = df[\"Document\"].str.split().str.len()\n",
    "\n",
    "print(\"Estatísticas da contagem de palavras:\")\n",
    "print(df[\"word_count\"].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df,\n",
    "    x=\"word_count\",\n",
    "    nbins=50,\n",
    "    title=\"Distribuição da Quantidade de Palavras por Ticket\",\n",
    "    color_discrete_sequence=[\"#66c2a5\"]\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Quantidade de Palavras\",\n",
    "    yaxis_title=\"Frequência\",\n",
    "    height=400\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentis para entender melhor a distribuição\n",
    "percentiles = [50, 75, 90, 95, 99]\n",
    "print(\"Percentis de palavras por ticket:\")\n",
    "for p in percentiles:\n",
    "    value = df[\"word_count\"].quantile(p/100)\n",
    "    print(f\"  {p}%: {value:.0f} palavras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussão: Distribuição do Tamanho dos Textos\n",
    "\n",
    "A distribuição apresenta assimetria positiva:\n",
    "- A mediana é significativamente menor que a média, indicando que a maioria dos tickets é curta\n",
    "- Existem outliers com textos muito longos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(\n",
    "    df,\n",
    "    x=\"Topic_group\",\n",
    "    y=\"word_count\",\n",
    "    color=\"Topic_group\",\n",
    "    title=\"Distribuição de Palavras por Classe\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Classe\",\n",
    "    yaxis_title=\"Quantidade de Palavras\",\n",
    "    showlegend=False,\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatísticas por classe\n",
    "stats_by_class = df.groupby(\"Topic_group\")[\"word_count\"].agg([\"mean\", \"median\", \"std\"]).round(1)\n",
    "stats_by_class = stats_by_class.sort_values(\"mean\", ascending=False)\n",
    "stats_by_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Análise de Palavras Frequentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = \" \".join(df[\"Document\"]).lower().split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "print(f\"Vocabulário total: {len(word_freq):,} palavras únicas\")\n",
    "print(f\"Total de palavras: {len(all_words):,}\")\n",
    "\n",
    "top_words = pd.DataFrame(word_freq.most_common(30), columns=[\"Palavra\", \"Frequência\"])\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    top_words,\n",
    "    x=\"Frequência\",\n",
    "    y=\"Palavra\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Top 30 Palavras Mais Frequentes\",\n",
    "    color=\"Frequência\",\n",
    "    color_continuous_scale=\"Viridis\"\n",
    ")\n",
    "fig.update_layout(\n",
    "    yaxis={\"categoryorder\": \"total ascending\"},\n",
    "    height=700,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Palavras Frequentes por Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words_by_class(df, class_name, n=10):\n",
    "    class_text = \" \".join(df[df[\"Topic_group\"] == class_name][\"Document\"]).lower().split()\n",
    "    return Counter(class_text).most_common(n)\n",
    "\n",
    "classes = df[\"Topic_group\"].unique()\n",
    "top_words_by_class = []\n",
    "\n",
    "for cls in classes:\n",
    "    for word, freq in get_top_words_by_class(df, cls, 10):\n",
    "        top_words_by_class.append({\"Classe\": cls, \"Palavra\": word, \"Frequência\": freq})\n",
    "\n",
    "df_top_words = pd.DataFrame(top_words_by_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    df_top_words,\n",
    "    x=\"Frequência\",\n",
    "    y=\"Palavra\",\n",
    "    color=\"Classe\",\n",
    "    facet_col=\"Classe\",\n",
    "    facet_col_wrap=4,\n",
    "    orientation=\"h\",\n",
    "    title=\"Top 10 Palavras por Classe\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2,\n",
    "    height=800\n",
    ")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Exemplos de Tickets por Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls in sorted(df[\"Topic_group\"].unique()):\n",
    "    count = len(df[df[\"Topic_group\"] == cls])\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CLASSE: {cls.upper()} ({count:,} tickets - {pct:.1f}%)\")\n",
    "    print(\"=\"*80)\n",
    "    samples = df[df[\"Topic_group\"] == cls].sample(2, random_state=42)\n",
    "    for i, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "        text = row['Document']\n",
    "        truncated = text[:300] + \"...\" if len(text) > 300 else text\n",
    "        print(f\"\\n[Exemplo {i}]\")\n",
    "        print(truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Resumo da Análise Exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RESUMO DA ANÁLISE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n[Dataset]\")\n",
    "print(f\"  Total de tickets: {len(df):,}\")\n",
    "print(f\"  Número de classes: {df['Topic_group'].nunique()}\")\n",
    "print(f\"  Sem valores nulos ou duplicados\")\n",
    "\n",
    "print(f\"\\n[Texto]\")\n",
    "print(f\"  Média de palavras: {df['word_count'].mean():.1f}\")\n",
    "print(f\"  Mediana de palavras: {df['word_count'].median():.1f}\")\n",
    "print(f\"  Máximo de palavras: {df['word_count'].max()}\")\n",
    "\n",
    "print(f\"\\n[Distribuição das Classes]\")\n",
    "for _, row in class_counts.iterrows():\n",
    "    print(f\"  {row['Classe']}: {row['Quantidade']:,} ({row['Percentual']}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusões e Escolha da Abordagem\n",
    "\n",
    "Com base na análise exploratória, identificamos características que guiam a escolha da abordagem de classificação:\n",
    "\n",
    "**Características do dataset:**\n",
    "- **~48k tickets rotulados** - Dataset grande com labels confiáveis\n",
    "- **8 classes com desbalanceamento** - Classes minoritárias (Administrative rights ~3.7%) precisam de tratamento especial\n",
    "- **Textos curtos** - Mediana de ~26 palavras permite processar múltiplos exemplos no contexto\n",
    "- **Padrões de vocabulário distintos** - Cada classe tem palavras-chave características (ex: \"card\" para Access, \"po/purchase\" para Purchase)\n",
    "\n",
    "**Abordagem escolhida: RAG + LLM**\n",
    "\n",
    "Utilizamos **RAG (Retrieval Augmented Generation)** combinado com um **LLM (Large Language Model)**:\n",
    "\n",
    "1. **Retrieval:** Buscar tickets similares no dataset de treino usando embeddings semânticos\n",
    "2. **Augmented:** Enriquecer o prompt com exemplos relevantes\n",
    "3. **Generation:** LLM classifica e justifica baseado no contexto\n",
    "\n",
    "**Por que RAG?**\n",
    "\n",
    "| Alternativa | Limitação |\n",
    "|-------------|-----------|\n",
    "| Fine-tuning de LLM | Requer recursos computacionais significativos e risco de overfitting |\n",
    "| ML tradicional (TF-IDF + SVM) | Boa precisão, mas não gera justificativas naturais |\n",
    "| Zero-shot com LLM | Perde a riqueza dos exemplos reais do dataset |\n",
    "\n",
    "**Vantagens do RAG para este problema:**\n",
    "\n",
    "1. **Aproveita o dataset grande:** Os ~48k tickets rotulados fornecem exemplos relevantes para qualquer novo ticket\n",
    "2. **Lida com desbalanceamento:** Classes minoritárias se beneficiam de exemplos específicos recuperados\n",
    "3. **Textos curtos facilitam:** A mediana de 26 palavras permite incluir vários exemplos no prompt sem exceder limites\n",
    "4. **Justificativas de qualidade:** LLM vê exemplos reais e pode referenciar padrões similares\n",
    "5. **Explicável:** Podemos inspecionar quais exemplos influenciaram cada decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Carregamento e Preparação dos Dados\n",
    "\n",
    "### Amostragem Balanceada\n",
    "\n",
    "Para avaliação, utilizamos **amostragem balanceada** em vez de estratificada:\n",
    "\n",
    "| Tipo | Descrição |\n",
    "|------|-----------|\n",
    "| **Estratificada** | Mantém a proporção original (classes raras têm poucas amostras) |\n",
    "| **Balanceada** | Mesmo número de amostras por classe |\n",
    "\n",
    "**Por que balanceada?**\n",
    "1. **Classes minoritárias importam** - Um classificador que falha em classes raras não é aceitável\n",
    "2. **Métricas confiáveis** - 25 amostras por classe permitem avaliação estatisticamente significativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.data import load_dataset, train_test_split_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset\n",
    "df, classes = load_dataset()\n",
    "print(f\"Total de tickets: {len(df):,}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes obtidas do dataset\n",
    "print(f\"Classes ({len(classes)}):\")\n",
    "for c in classes:\n",
    "    print(f\"  - {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.config import TEST_SIZE\n",
    "\n",
    "# Split balanceado: treino para RAG, teste para avaliação\n",
    "# Amostragem balanceada garante o mesmo número de tickets por classe (200 / 8 = 25)\n",
    "train_df, test_df = train_test_split_balanced(df, test_size=TEST_SIZE)\n",
    "\n",
    "print(f\"Treino: {len(train_df):,} tickets\")\n",
    "print(f\"Teste:  {len(test_df)} tickets\")\n",
    "print(f\"\\nDistribuição no teste:\")\n",
    "print(test_df[\"Topic_group\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG - Retrieval de Tickets Similares\n",
    "\n",
    "### Como funciona\n",
    "\n",
    "1. **Embeddings:** Cada ticket é convertido em um vetor de 384 dimensões usando o modelo `all-MiniLM-L6-v2` (sentence-transformers). Este modelo foi treinado para capturar similaridade semântica.\n",
    "\n",
    "2. **Similaridade de cosseno:** Para encontrar tickets similares, calculamos o cosseno do ângulo entre vetores. Valores próximos de 1 indicam alta similaridade.\n",
    "\n",
    "3. **Tickets representativos:** Para cada classe, calculamos o \"centróide\" (média dos embeddings) e selecionamos o ticket mais próximo. Isso nos dá um exemplo típico de cada classe.\n",
    "\n",
    "### Componentes\n",
    "\n",
    "- `TicketRetriever.index()`: Gera embeddings para todos os tickets de treino\n",
    "- `TicketRetriever.retrieve()`: Busca os K tickets mais similares\n",
    "- `TicketRetriever.compute_representatives()`: Calcula tickets representativos por classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.rag import TicketRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexar tickets de treino\n",
    "retriever = TicketRetriever()\n",
    "retriever.index(train_df)\n",
    "\n",
    "# Calcular tickets representativos de cada classe (centróides)\n",
    "representatives = retriever.compute_representatives()\n",
    "print(f\"\\nTickets representativos calculados para {len(representatives)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar retrieval com um ticket do conjunto de teste\n",
    "test_ticket = test_df.iloc[1]\n",
    "query = test_ticket[\"Document\"]\n",
    "true_class = test_ticket[\"Topic_group\"]\n",
    "\n",
    "similar = retriever.retrieve(query, k=5)\n",
    "\n",
    "print(f\"Ticket de teste (classe real: {true_class}):\")\n",
    "print(f\"{query}\\n\")\n",
    "print(\"Tickets similares recuperados:\")\n",
    "for i, ticket in enumerate(similar, 1):\n",
    "    print(f\"\\n{i}. [{ticket['class']}] (score: {ticket['score']:.3f})\")\n",
    "    print(f\"   {ticket['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar tickets representativos\n",
    "print(\"Tickets representativos (mais próximos do centróide de cada classe):\\n\")\n",
    "for class_name in sorted(representatives.keys()):\n",
    "    t = representatives[class_name]\n",
    "    print(f\"[{class_name}] (score: {t['score']:.3f})\")\n",
    "    print(f\"   {t['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Design dos Prompts\n",
    "\n",
    "O prompt é a interface entre nosso sistema e a LLM. Um bom design de prompt é crucial para obter classificações precisas e justificativas úteis.\n",
    "\n",
    "### Estrutura do Prompt\n",
    "\n",
    "O prompt é dividido em duas partes:\n",
    "\n",
    "1. **System prompt:** Define o papel da LLM (classificador), lista as classes válidas e especifica o formato de saída (JSON)\n",
    "\n",
    "2. **User prompt:** Contém o ticket a classificar e os exemplos de contexto\n",
    "\n",
    "### Parâmetros Configuráveis\n",
    "\n",
    "| Parâmetro | Descrição | Valor Padrão |\n",
    "|-----------|-----------|---------------|\n",
    "| `K_SIMILAR` | Número de tickets similares do RAG | 5 |\n",
    "| `reference_tickets` | Tickets representativos por classe | 1 por classe |\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "- **Mais exemplos similares:** Melhor contexto para classificação, mas aumenta tokens e custo\n",
    "- **Tickets de referência:** Garante diversidade de classes, essencial para justificativas comparativas\n",
    "- **Muitos exemplos:** Pode \"poluir\" o contexto e confundir a LLM\n",
    "\n",
    "Vamos visualizar como os prompts são gerados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.prompts import build_system_prompt, build_user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 1: System Prompt\n",
    "system_prompt = build_system_prompt(classes)\n",
    "print(\"=== SYSTEM PROMPT ===\\n\")\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 2: User Prompt COM tickets de referência\n",
    "user_prompt_with_refs = build_user_prompt(query, similar, representatives)\n",
    "print(\"=== USER PROMPT (com tickets de referência) ===\\n\")\n",
    "print(user_prompt_with_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 3: User Prompt SEM tickets de referência\n",
    "# Útil quando queremos usar menos tokens ou quando os similares já são suficientes\n",
    "user_prompt_no_refs = build_user_prompt(query, similar, reference_tickets=None)\n",
    "print(\"=== USER PROMPT (sem tickets de referência) ===\\n\")\n",
    "print(user_prompt_no_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação de Uso de Tokens\n",
    "\n",
    "A tabela abaixo mostra o impacto real de cada parâmetro no consumo de tokens. Isso ajuda a escolher a configuração ideal considerando o trade-off entre qualidade do contexto e custo/latência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Usar tokenizer cl100k_base (compatível com GPT-4, GPT-3.5-turbo, etc.)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Conta tokens usando o tokenizer cl100k_base.\"\"\"\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "# Comparar diferentes configurações de prompt\n",
    "# K=5 sem refs é a baseline (configuração mínima recomendada)\n",
    "configs = [\n",
    "    (\"K=5, sem refs\", build_user_prompt(query, similar[:5], None)),\n",
    "    (\"K=5, com refs\", build_user_prompt(query, similar[:5], representatives)),\n",
    "    (\"K=3, sem refs\", build_user_prompt(query, similar[:3], None)),\n",
    "    (\"K=3, com refs\", build_user_prompt(query, similar[:3], representatives)),\n",
    "    (\"K=1, sem refs\", build_user_prompt(query, similar[:1], None)),\n",
    "    (\"K=1, com refs\", build_user_prompt(query, similar[:1], representatives)),\n",
    "]\n",
    "\n",
    "# Tabela comparativa\n",
    "system_tokens = count_tokens(system_prompt)\n",
    "print(f\"System prompt: {system_tokens} tokens (fixo)\\n\")\n",
    "print(f\"{'Configuração':<16} | {'User Prompt':>12} | {'Total':>8} | {'vs baseline':>12}\")\n",
    "print(\"-\" * 58)\n",
    "baseline = None\n",
    "for name, prompt in configs:\n",
    "    user_tokens = count_tokens(prompt)\n",
    "    total = system_tokens + user_tokens\n",
    "    if baseline is None:\n",
    "        baseline = total\n",
    "        diff = \"(base)\"\n",
    "    else:\n",
    "        diff = f\"{(total - baseline) / baseline * 100:+.0f}%\"\n",
    "    print(f\"{name:<16} | {user_tokens:>12} | {total:>8} | {diff:>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classificação com LLM\n",
    "\n",
    "Com o prompt construído, enviamos para a LLM. O sistema suporta qualquer API compatível com OpenAI configurada via variáveis de ambiente.\n",
    "\n",
    "**Requer:** variáveis de ambiente `LLM_BASE_URL` e `LLM_MODEL` (ver `.env.example`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.llm import TicketClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar classificador\n",
    "classifier = TicketClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classificar o ticket de teste\n",
    "details = classifier.classify(query, similar, classes, reference_tickets=representatives)\n",
    "\n",
    "print(f\"Classe real: {true_class}\")\n",
    "print(f\"Classe predita: {details.result.classe}\")\n",
    "print(f\"Justificativa: {details.result.justificativa}\")\n",
    "if details.reasoning:\n",
    "    print(f\"\\nReasoning:\")\n",
    "    print(details.reasoning)\n",
    "print(f\"\\nCorreto: {details.result.classe == true_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo de Resposta com Reasoning\n",
    "\n",
    "**Sem reasoning:**\n",
    "```json\n",
    "{\n",
    "  \"classe\": \"Access\",\n",
    "  \"justificativa\": \"O ticket menciona problemas com cartão de acesso.\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Com reasoning:**\n",
    "```json\n",
    "{\n",
    "  \"classe\": \"Access\",\n",
    "  \"justificativa\": \"O ticket descreve problemas com 'card key' e 'door access'...\",\n",
    "  \"reasoning\": \"Analisei o ticket e identifiquei palavras-chave como 'card key', 'door' e 'access'...\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### Configuração\n",
    "\n",
    "```bash\n",
    "# .env\n",
    "LLM_MODEL=xiaomi/mimo-v2-flash:free\n",
    "LLM_REASONING_EFFORT=medium  # low, medium, ou high\n",
    "```\n",
    "\n",
    "Via CLI:\n",
    "```bash\n",
    "uv run python main.py --reasoning medium\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de Reasoning\n",
    "\n",
    "Ativamos o **modo reasoning** do modelo (`LLM_REASONING_EFFORT=medium`) para melhorar a qualidade das classificações.\n",
    "\n",
    "#### Benefícios\n",
    "\n",
    "| Aspecto | Impacto do Reasoning |\n",
    "|---------|---------------------|\n",
    "| **Casos ambíguos** | O modelo \"pensa\" antes de decidir, analisando nuances |\n",
    "| **Justificativas** | Explicações mais detalhadas do processo de decisão |\n",
    "| **Precisão** | Reduz erros entre classes similares |\n",
    "| **Transparência** | Campo `reasoning` mostra o raciocínio completo |\n",
    "\n",
    "#### Trade-offs\n",
    "\n",
    "Ao ativar reasoning, enfrentamos:\n",
    "| Aspecto | Trade-off |\n",
    "|---------|-----------|\n",
    "| **Tokens** | Aumento no número de tokens e custo por chamada |\n",
    "| **Latência** | Respostas mais lentas devido ao processamento adicional |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Escolha do Modelo LLM\n",
    "\n",
    "- Disponível gratuitamente via OpenRouter\n",
    "- Suporte nativo a reasoning\n",
    "- Performance competitiva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitetura da Solução\n",
    "\n",
    "O diagrama abaixo ilustra o fluxo completo de classificação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Criar diagrama de arquitetura\n",
    "fig = go.Figure()\n",
    "\n",
    "# Definir as etapas do pipeline\n",
    "steps = [\n",
    "    (\"Ticket\", \"#e1f5fe\", \"Texto de entrada\"),\n",
    "    (\"Embedding\", \"#fff3e0\", \"all-MiniLM-L6-v2\\n384 dimensões\"),\n",
    "    (\"Retrieval\", \"#e8f5e9\", \"K similares +\\nRepresentativos\"),\n",
    "    (\"Prompt\", \"#fce4ec\", \"System + User\\n+ Contexto RAG\"),\n",
    "    (\"LLM\", \"#f3e5f5\", \"API OpenAI-compatible\\n+ Retry JSON\"),\n",
    "    (\"Output\", \"#e0f2f1\", '{\"classe\": \"...\",\\n\"justificativa\": \"...\"}'),\n",
    "]\n",
    "\n",
    "# Posições\n",
    "x_positions = list(range(len(steps)))\n",
    "y_pos = 0.5\n",
    "\n",
    "# Adicionar caixas e textos\n",
    "for i, (name, color, desc) in enumerate(steps):\n",
    "    # Caixa\n",
    "    fig.add_shape(\n",
    "        type=\"rect\",\n",
    "        x0=i - 0.4, x1=i + 0.4,\n",
    "        y0=0.2, y1=0.8,\n",
    "        fillcolor=color,\n",
    "        line=dict(color=\"#333\", width=2),\n",
    "    )\n",
    "    # Nome da etapa\n",
    "    fig.add_annotation(\n",
    "        x=i, y=0.65,\n",
    "        text=f\"<b>{name}</b>\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=14),\n",
    "    )\n",
    "    # Descrição\n",
    "    fig.add_annotation(\n",
    "        x=i, y=0.38,\n",
    "        text=desc,\n",
    "        showarrow=False,\n",
    "        font=dict(size=10),\n",
    "        align=\"center\",\n",
    "    )\n",
    "    # Seta para próxima etapa\n",
    "    if i < len(steps) - 1:\n",
    "        fig.add_annotation(\n",
    "            x=i + 0.5, y=0.5,\n",
    "            ax=i + 0.42, ay=0.5,\n",
    "            xref=\"x\", yref=\"y\",\n",
    "            axref=\"x\", ayref=\"y\",\n",
    "            showarrow=True,\n",
    "            arrowhead=2,\n",
    "            arrowsize=1.5,\n",
    "            arrowcolor=\"#333\",\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text=\"Arquitetura do Pipeline RAG\", x=0.5, font=dict(size=16)),\n",
    "    xaxis=dict(visible=False, range=[-0.6, len(steps) - 0.4]),\n",
    "    yaxis=dict(visible=False, range=[0, 1]),\n",
    "    height=250,\n",
    "    margin=dict(l=20, r=20, t=50, b=20),\n",
    "    plot_bgcolor=\"white\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pipeline Completo com LangGraph\n",
    "\n",
    "O LangGraph orquestra todo o fluxo de classificação em um grafo de estados com 4 nós:\n",
    "\n",
    "| Nó | Função |\n",
    "|----|--------|\n",
    "| **embed** | Gera embedding do ticket (384 dimensões) |\n",
    "| **retrieve** | Busca K tickets similares usando o embedding |\n",
    "| **build_prompt** | Constrói system e user prompts com contexto RAG |\n",
    "| **classify** | Chama a LLM e processa a resposta JSON |\n",
    "\n",
    "Abaixo visualizamos a estrutura do grafo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from classifier.graph import create_graph\n",
    "\n",
    "# Criar o grafo para visualização\n",
    "pipeline = create_graph(retriever, classifier, classes, representatives)\n",
    "\n",
    "# Visualizar a estrutura do grafo LangGraph\n",
    "display(Image(pipeline.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.graph import classify_ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classificar usando o pipeline completo\n",
    "# A função classify_ticket encapsula todo o fluxo: retrieve → classify\n",
    "details_graph = classify_ticket(\n",
    "    ticket=query,\n",
    "    retriever=retriever,\n",
    "    classifier=classifier,\n",
    "    classes=classes,\n",
    "    reference_tickets=representatives,\n",
    ")\n",
    "\n",
    "print(f\"Classe real: {true_class}\")\n",
    "print(f\"Classe predita: {details_graph.result.classe}\")\n",
    "print(f\"Justificativa: {details_graph.result.justificativa}\")\n",
    "if details_graph.reasoning:\n",
    "    print(f\"\\nReasoning:\")\n",
    "    print(details_graph.reasoning)\n",
    "print(f\"\\nCorreto: {details_graph.result.classe == true_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Avaliação do Classificador\n",
    "\n",
    "Agora vamos avaliar o desempenho do classificador nos 200 tickets de teste.\n",
    "\n",
    "### Métricas Utilizadas\n",
    "\n",
    "- **Accuracy:** Proporção de classificações corretas\n",
    "- **F1 Macro:** Média não-ponderada do F1 por classe (trata todas as classes igualmente, importante para datasets desbalanceados)\n",
    "- **Cohen's Kappa:** Mede concordância além do acaso (valores próximos de 1 indicam excelente concordância)\n",
    "- **MCC (Matthews Correlation Coefficient):** Métrica robusta para classificação multi-classe\n",
    "- **Confusion Matrix:** Visualiza erros de classificação entre classes\n",
    "\n",
    "### Tratamento de Erros\n",
    "\n",
    "LLMs podem ocasionalmente retornar JSON malformado, ou a API pode apresentar erros. Para lidar com isso:\n",
    "\n",
    "**Erros de JSON (parsing):**\n",
    "1. **Primeira tentativa:** Envia o prompt normal para a LLM\n",
    "2. **Se JSON inválido:** Continua a conversa adicionando a resposta do assistant e um prompt de correção:\n",
    "   ```\n",
    "   Sua resposta anterior não está no formato JSON válido.\n",
    "   Por favor, responda APENAS com JSON válido no formato:\n",
    "   {\"classe\": \"<categoria>\", \"justificativa\": \"<explicação>\"}\n",
    "   ```\n",
    "3. **Segunda tentativa:** LLM recebe o contexto completo da conversa e tenta corrigir\n",
    "4. **Se falhar novamente:** O ticket é marcado como erro e reportado separadamente\n",
    "\n",
    "**Erros de API:**\n",
    "- Erros de autenticação (401), rate limit (429), ou outros erros da API são capturados\n",
    "- O ticket é marcado como erro com o motivo específico\n",
    "- A avaliação continua com os próximos tickets\n",
    "\n",
    "Este approach garante que:\n",
    "- Um erro pontual não interrompe toda a avaliação\n",
    "- A LLM tem chance de se corrigir com contexto adicional\n",
    "- Erros persistentes são documentados para análise posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.metrics import evaluate, print_report, plot_confusion_matrix, plot_per_class_metrics\n",
    "from classifier.llm import ClassificationError\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classificar todos os tickets de teste\n",
    "y_true = []\n",
    "y_pred = []\n",
    "results = []\n",
    "classification_errors = []\n",
    "total_tokens = {\"prompt\": 0, \"completion\": 0, \"total\": 0}\n",
    "\n",
    "print(f\"Classificando {len(test_df)} tickets de teste...\\n\")\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Classificando\"):\n",
    "    ticket_text = row[\"Document\"]\n",
    "    true_label = row[\"Topic_group\"]\n",
    "    \n",
    "    try:\n",
    "        details = classify_ticket(\n",
    "            ticket=ticket_text,\n",
    "            retriever=retriever,\n",
    "            classifier=classifier,\n",
    "            classes=classes,\n",
    "            reference_tickets=representatives,\n",
    "        )\n",
    "        \n",
    "        y_true.append(true_label)\n",
    "        y_pred.append(details.result.classe)\n",
    "        results.append({\n",
    "            \"ticket\": ticket_text,\n",
    "            \"true\": true_label,\n",
    "            \"pred\": details.result.classe,\n",
    "            \"justificativa\": details.result.justificativa,\n",
    "            \"correct\": true_label == details.result.classe,\n",
    "            \"token_usage\": {\n",
    "                \"prompt_tokens\": details.token_usage.prompt_tokens,\n",
    "                \"completion_tokens\": details.token_usage.completion_tokens,\n",
    "                \"total_tokens\": details.token_usage.total_tokens,\n",
    "            },\n",
    "            \"similar_tickets\": details.similar_tickets,\n",
    "        })\n",
    "        total_tokens[\"prompt\"] += details.token_usage.prompt_tokens\n",
    "        total_tokens[\"completion\"] += details.token_usage.completion_tokens\n",
    "        total_tokens[\"total\"] += details.token_usage.total_tokens\n",
    "    except ClassificationError as e:\n",
    "        classification_errors.append({\n",
    "            \"ticket\": ticket_text,\n",
    "            \"true\": true_label,\n",
    "            \"reason\": e.reason,\n",
    "            \"raw_response\": e.raw_response,\n",
    "        })\n",
    "\n",
    "print(f\"\\nClassificação concluída!\")\n",
    "print(f\"Classificados com sucesso: {len(results)}\")\n",
    "if classification_errors:\n",
    "    print(f\"Erros de classificação (JSON inválido): {len(classification_errors)}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TOKEN USAGE SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Prompt tokens:     {total_tokens['prompt']:,}\")\n",
    "print(f\"Completion tokens: {total_tokens['completion']:,}\")\n",
    "print(f\"Total tokens:      {total_tokens['total']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas\n",
    "metrics = evaluate(y_true, y_pred, classes)\n",
    "print_report(metrics, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas por Classe\n",
    "\n",
    "O gráfico abaixo mostra precision, recall e F1-score para cada classe, facilitando a identificação visual de quais classes têm melhor/pior desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de métricas por classe\n",
    "plot_per_class_metrics(y_true, y_pred, classes, title=\"Precision, Recall e F1-Score por Classe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar confusion matrix\n",
    "plot_confusion_matrix(metrics[\"confusion_matrix\"], classes, title=\"Confusion Matrix - Classificador RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Normalizada\n",
    "\n",
    "A versão normalizada mostra percentagens em vez de contagens absolutas, facilitando a identificação de taxas de erro por classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix normalizada (percentagens)\n",
    "plot_confusion_matrix(\n",
    "    metrics[\"confusion_matrix\"], \n",
    "    classes, \n",
    "    title=\"Confusion Matrix - Normalizada por True Label\",\n",
    "    normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de Erros\n",
    "\n",
    "Existem dois tipos de erros na avaliação:\n",
    "\n",
    "1. **Erros de classificação:** A LLM retornou um JSON válido, mas a classe predita difere da classe real\n",
    "2. **Erros de parsing:** A LLM não retornou JSON válido mesmo após retry (tickets não incluídos nas métricas)\n",
    "\n",
    "Abaixo analisamos os erros de classificação para entender os padrões de confusão entre classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar erros\n",
    "errors = [r for r in results if not r[\"correct\"]]\n",
    "print(f\"Total de erros: {len(errors)} de {len(results)} ({100 * len(errors) / len(results):.1f}%)\\n\")\n",
    "\n",
    "# Mostrar alguns exemplos de erros\n",
    "print(\"=\" * 80)\n",
    "print(\"EXEMPLOS DE ERROS DE CLASSIFICAÇÃO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, error in enumerate(errors[:5], 1):\n",
    "    print(f\"\\n--- Erro {i} ---\")\n",
    "    print(f\"Classe real:     {error['true']}\")\n",
    "    print(f\"Classe predita:  {error['pred']}\")\n",
    "    print(f\"Justificativa:   {error['justificativa']}\")\n",
    "    print(f\"Ticket:          {error['ticket'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição de erros por classe\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Distribuição de erros por classe real:\")\n",
    "error_by_true = Counter(e[\"true\"] for e in errors)\n",
    "for cls, count in sorted(error_by_true.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {cls}: {count} erros\")\n",
    "\n",
    "print(\"\\nConfusões mais comuns (real → predito):\")\n",
    "confusion_pairs = Counter((e[\"true\"], e[\"pred\"]) for e in errors)\n",
    "for (true, pred), count in confusion_pairs.most_common(10):\n",
    "    print(f\"  {true} → {pred}: {count}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuição de Erros por Classe\n",
    "\n",
    "Visualização gráfica dos erros por classe real, facilitando a identificação de quais classes são mais difíceis de classificar corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Gráfico de erros por classe\n",
    "if errors:\n",
    "    error_data = [{\"Classe\": cls, \"Erros\": count} for cls, count in error_by_true.items()]\n",
    "    fig = px.bar(\n",
    "        error_data, \n",
    "        x=\"Erros\", \n",
    "        y=\"Classe\", \n",
    "        orientation=\"h\",\n",
    "        title=\"Erros por Classe Real\",\n",
    "        color=\"Erros\",\n",
    "        color_continuous_scale=\"Reds\"\n",
    "    )\n",
    "    fig.update_layout(height=350, showlegend=False)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Nenhum erro de classificação para exibir!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de Scores de Similaridade\n",
    "\n",
    "Comparação dos scores de similaridade entre predições corretas e incorretas. Scores mais baixos em predições incorretas indicam que o retriever teve dificuldade em encontrar exemplos relevantes. Se os scores são similares, o problema está na classificação do LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calcular média dos scores de similaridade por predição\n",
    "def mean_similarity(result):\n",
    "    scores = [t['score'] for t in result['similar_tickets']]\n",
    "    return np.mean(scores) if scores else 0\n",
    "\n",
    "scores_correct = [mean_similarity(r) for r in results if r['correct']]\n",
    "scores_incorrect = [mean_similarity(r) for r in results if not r['correct']]\n",
    "\n",
    "# Box plot comparativo\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(y=scores_correct, name='Corretos', marker_color='#2ca02c'))\n",
    "fig.add_trace(go.Box(y=scores_incorrect, name='Incorretos', marker_color='#d62728'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Distribuição de Scores de Similaridade (Média dos K Similares)',\n",
    "    yaxis_title='Score de Similaridade',\n",
    "    height=400\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Estatísticas\n",
    "print(f\"Corretos:   média={np.mean(scores_correct):.3f}, mediana={np.median(scores_correct):.3f}, n={len(scores_correct)}\")\n",
    "if scores_incorrect:\n",
    "    print(f\"Incorretos: média={np.mean(scores_incorrect):.3f}, mediana={np.median(scores_incorrect):.3f}, n={len(scores_incorrect)}\")\n",
    "else:\n",
    "    print(\"Nenhuma predição incorreta para análise.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ticket-classifier (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
